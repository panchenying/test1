
#HYPOTHESIS
I will be answering the question: What childhood (8-9 years) risk factors predict offending?
Based on this a priori hypothesis from readings I've done, I will be looking at variables related to problem child behaviour, low intelligence, juvenile offending, sibling delinquency, and neglect.

##loading multiple packages with the pacman package
```{r}
library(pacman)
p_load(dplyr, ggplot2, magrittr, psych, tidyverse, haven, broom, purrr, vcd, DHARMa, caret,
       kableExtra, car, leaps, glmnet, Metrics, magick, bootES, randomForest, tree, 
       gbm, class, e1071, stringr, knitr)
```
##loading data using haven to read SPSS files
```{r}
conviction <- read_spss("conviction_data.sav")
delinquency <- read_spss("Cambridge_delinquency.sav")
```

#QUESTION 1
##Subset dataset
```{r}
delinquency <- delinquency %>%  
  select(v53, v137, v123, v119)
```
##Merge dataset
```{r}
conviction_spread <- spread(conviction, agecat, convicted) #spreading conviction data into wide format
delinquency$icpsr_seq_id_number<-c(1:411) #creating a common column key before merging
deliquency_joined <- left_join(x = delinquency, y = conviction_spread, by = "icpsr_seq_id_number") #merging by icpsr_seq_id_number
```
##Subset merged dataset
```{r}
delinquency <- deliquency_joined %>%  
  select(icpsr_seq_id_number, v53, v137, v123, v119, convicted_as_juvenile, convicted_as_adult)
```
##Renaming variables
```{r}
delinquency <- delinquency %>% rename(id = icpsr_seq_id_number, conduct_disorder = v53,
                                      sibling_disturbance = v137, physical_neglect = v123, 
                                      iq = v119)
p_load(sjlabelled) #label package
delinquency <- remove_all_labels(delinquency) #removing all labels as they are no longer necessary (redundent)
```

#QUESTION 2
##Exploring data
```{r}
#simple summary statistics
delinquency %>% 
  describeBy(group = delinquency$convicted_as_adult, mat = T) %>% 
  kable() %>%
  kable_styling() #tidying data into table
```

Things to note in the summary statistics is that most participants don't have conduct disorder and most don't have a deliquent sibling.

```{r}
#correlations
pairs.panels(delinquency)
```

looking at correlations, we see that being convicted as a juvenile and being convicted as an adult has a moderate positive relationship. In other words, it is more likely to be convicted as an adult if the participant has been convicted as a juvenile.
none of the other variables are too highly correlated, meaning that there are no redundent variables.

```{r}
#bar graph (adult convictions)
ggplot(delinquency, aes(x = convicted_as_adult)) + 
  geom_bar(fill = "darkgrey", na.rm = TRUE) + #removing the NAs in the dataset
  xlab("convicted_as_adult") + ylab("Number of participants") +
  ggtitle("Number of participants with adult convictions") #adding a title to the bar graph

#bar graph (juvenile convictions)
ggplot(delinquency, aes(x = convicted_as_juvenile)) + 
  geom_bar(fill = "lightgrey", na.rm = TRUE) + #removing the NAs in the dataset
  xlab("convicted_as_juvenile") + ylab("Number of participants") +
  ggtitle("Number of participants with juvenile convictions") #adding a title to the bar graph
```

from the bar graph, we can tell that there are more participants not convicted as adults than those convicted as adults. similarly, there are more participants who were not convicted as juveniles than those convicted a juveniles. there are actually less people convicted as juveniles compared to adults.

```{r}
#mosaic plot (adult conviction, physical neglect, and conduct disorder)
mosaic(~ convicted_as_adult + physical_neglect + conduct_disorder, data = delinquency)
```

Looking at the mosaic plot to understand the relations between our variables and adult convictions. We can see that there is a strong relation between conduct disorder and physical neglect. In other words, those who are neglected tend to have conduct disorder. In terms of those who are convicted as adults, they tend to have conduct disorder but tend to not be neglected, which is interested considering the readings.

```{r}
#mosaic plot (adult conviction, juvenile conviction, and sibling disturbance)
mosaic(~ convicted_as_adult + convicted_as_juvenile + sibling_disturbance, data = delinquency)
```

from this mosaic plot, we can see that those convicted as juveniles will most likely be convicted as adults, in addition, those convicted as adults and juveniles will most likely have a sibling who is also delinquent. 

#QUESTION 3
##Training and test data
```{r warning=FALSE}
#creating training and testing sets
set.seed(135)
traindata <- sample_frac(delinquency, 0.75) #75% of the delinquency data will be allocated to training data
testdata <- setdiff(delinquency, traindata) #the difference, 25%, will be allocated to testing data
```
##Remove NA and set convictions as factors
```{r}
#Defining adult convictions as factors in the training data
traindata %<>% 
  mutate(convicted_as_adult = factor(convicted_as_adult)) %>% 
  na.omit() #omit NAs from the data

#Same thing for the test set
testdata %<>% 
  mutate(convicted_as_adult = factor(convicted_as_adult)) %>% 
  na.omit() #omit NAs from the data
```

```{r}
#mse function to test model predictive accuracy
mse_fun <- function(a_model) #input a model
    mean(a_model$residuals^2) #the mean of the square root of the residuals 
```


##Buidling the logistic regression model and building MSE function
```{r}
#logistic regression training model 1
trainmodel1 <- glm(convicted_as_adult ~ conduct_disorder + sibling_disturbance +
    physical_neglect + iq + convicted_as_juvenile, family = "binomial", data = traindata)
tidy(trainmodel1)  %>% #summary of training model1
  kable() %>%
  kable_styling() #tidying data into table
mse_fun(trainmodel1)

#logitsic regression test model 1
testmodel1 <- glm(convicted_as_adult ~ conduct_disorder + sibling_disturbance +
    physical_neglect + iq + convicted_as_juvenile, family = "binomial", data = testdata)
tidy(testmodel1)  %>% #summary of training model1
  kable() %>%
  kable_styling() #tidying data into table
mse_fun(testmodel1) #mse function
```
Our test model has the lower MSE, thus, it is a better model with smaller error.

```{r warning=FALSE}
#predicting accuracy of the training model
trainmodelpred <- predict(trainmodel1, type = "response")
confusionMatrix(as.factor(as.numeric(trainmodelpred >= 0.05)), #predictions cut off set at 0.05
                as.factor(traindata$convicted_as_adult)) %>% 
                tidy() %>% kable() %>%
                kable_styling() #tidy the data output
```


The trainging logistic regression model yielded a 71,37% accuracy in predictions with the training model


```{r warning=FALSE}
#looking at predicting accuracy of testing model
testmodelpred <- predict(trainmodel1, newdata = testdata, type = "response")
confusionMatrix(as.factor(as.numeric(testmodelpred >= 0.05)),
                as.factor(testdata$convicted_as_adult)) %>% 
                tidy() %>% kable() %>%
                kable_styling() #tidy the data output #tidy the data output
```


there is a 67,53% prediction accuracy with the testing model, although there is only a slight drop. The prediction accuracy is moderate (not very good)


##Building a tree model
```{r}
#making a tree!
traindata2 <- traindata %>%
select(-id) #exlcuding ID from the training dataset
traintree1 <- tree(convicted_as_adult ~ ., data = traindata2)
plot(traintree1) #plotting our tree
text(traintree1, pretty = 1) #adding text to our tree plot
```
```{r}
#predictive accuracy of tree on test data
traintreepred1 <- predict(traintree1, newdata = testdata, type = "class")
confusionMatrix(traintreepred1, testdata$convicted_as_adult) %>% 
  tidy() %>% kable() %>%
  kable_styling() #tidy the data output #tidy the data output
```

the predicitve accuracy is pretty good on our test data. It is 74%, which is higher than our logictic regression mode. We shall prune the tree to see if we can get a better accuracy.

##Pruning tree
```{r}
#pruning the tree
prunetree1 <- cv.tree(traintree1)
plot(prunetree1$size, prunetree1$dev, type = "b")
```

it seems that 2 variables is the best... but we'll go with 3 first

```{r}
#running the tree with only 3 variables
prune_traintree = prune.tree(traintree1, best = 3)
plot(prune_traintree)
text(prune_traintree, pretty = 1)
```
```{r}
#testing predictive accuracy of pruned tree
traintreepred2 <- predict(prune_traintree,
newdata = testdata, type = "class")
confusionMatrix(traintreepred2, testdata$convicted_as_adult) %>% 
  tidy() %>% kable() %>%
  kable_styling() #tidy the data output#tidy the data output
```

the accuracy has increased! to 84,42% which is a much better predictive accuracy.

##Bagging tree
```{r}
#Bagging - try to improve tree classification
traintreebag <- traindata %>%
select(-id) %$% #removing ID from dataset
randomForest(convicted_as_adult ~ ., data = ., mtry = 3)
#predicting with test data
traintreebag_pred <- predict(traintreebag, newdata = testdata, type = "class")
confusionMatrix(traintreebag_pred, testdata$convicted_as_adult) %>% 
  tidy() %>% kable() %>%
  kable_styling() #tidy the data output #tidy the data output
```

accuracy is similar to that of our first tree model, slightly higher though, and still higher than our logistic regression model.

##Random forest of trees
```{r}
#Random forest - try this to improve model
traindataf <- traindata %>%
select(-id) %$% #removing ID from our dataset
randomForest(convicted_as_adult ~ ., data = ., mtry = 3)
#predicitng with test data
traindataf_pred <- predict(traindataf, newdata = testdata, type = "class")
confusionMatrix(traindataf_pred, testdata$convicted_as_adult) %>% 
  tidy() %>% kable() %>%
  kable_styling() #tidy the data output #tidy the data output
```

accuracy is also similar to that of our tree bag, although slightly better

#CONCLUSION
In the end, our tree with 3 variables (convicted as juvenile and IQ) had the highest predicitve accuracy compared to our other trees and the logistic regression model.

#QUESTION 4
github link: (https://github.com/panchenying/test1.git)






